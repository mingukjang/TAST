Environment:
	Python: 3.7.4
	PyTorch: 1.8.1+cu102
	Torchvision: 0.9.1+cu102
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.20.3
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./dataset
	dataset: PACS
	holdout_fraction: 0.2
	hparams: {"backbone": "resnet50"}
	hparams_seed: 0
	output_dir: test_log
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: resnet50
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.1348383160  0.1613691932  0.2084221748  0.1880341880  0.1085329341  0.1077844311  0.2465012723  0.2560509554  0.0000000000  2.0145323277  0             0.5652303696 
0.8157413057  0.7946210269  0.9701492537  0.9401709402  0.9970059880  0.9760479042  0.9570610687  0.9312101911  7.1856287425  0.2431723745  300           0.3051173544 
0.8084197682  0.7946210269  0.9925373134  0.9487179487  0.9955089820  0.9730538922  0.9850508906  0.9426751592  14.371257485  0.0595255844  600           0.3056812501 
0.8639414277  0.8410757946  0.9962686567  0.9529914530  0.9985029940  0.9850299401  0.9888676845  0.9503184713  21.556886227  0.0358117026  900           0.3060787042 
0.8596705308  0.8679706601  0.9920042644  0.9658119658  0.9992514970  0.9700598802  0.9898218830  0.9439490446  28.742514970  0.0245259488  1200          0.3063416107 
0.8657718121  0.8606356968  0.9978678038  0.9572649573  0.9985029940  0.9670658683  0.9961832061  0.9528662420  35.928143712  0.0199237736  1500          0.3064441387 
0.8029286150  0.8117359413  0.9845415778  0.9487179487  0.9977544910  0.9670658683  0.9856870229  0.9528662420  43.113772455  0.0186860652  1800          0.3063919266 
0.7870652837  0.7921760391  0.9941364606  0.9423076923  0.9977544910  0.9580838323  0.9933206107  0.9515923567  50.299401197  0.0189342222  2100          0.3065292684 
0.8090298963  0.8288508557  0.9946695096  0.9636752137  0.9985029940  0.9700598802  0.9971374046  0.9592356688  57.485029940  0.0168044657  2400          0.3063244462 
0.8804148871  0.8997555012  0.9893390192  0.9444444444  0.9970059880  0.9640718563  0.9945928753  0.9439490446  64.670658682  0.0136970092  2700          0.3064418070 
0.8700427090  0.8728606357  0.9941364606  0.9529914530  0.9977544910  0.9730538922  0.9898218830  0.9375796178  71.856287425  0.0146767327  3000          0.3062377556 
0.8541793777  0.8361858191  0.9978678038  0.9636752137  1.0000000000  0.9790419162  0.9945928753  0.9528662420  79.041916167  0.0163570924  3300          0.3058830595 
0.8602806589  0.8435207824  0.9989339019  0.9572649573  0.9985029940  0.9760479042  0.9974554707  0.9605095541  86.227544910  0.0103801637  3600          0.3064088925 
0.8212324588  0.8092909535  0.9962686567  0.9401709402  0.9970059880  0.9670658683  0.9939567430  0.9426751592  93.413173652  0.0147323909  3900          0.3064320850 
0.8370957901  0.8410757946  0.9973347548  0.9551282051  0.9992514970  0.9670658683  0.9965012723  0.9515923567  100.59880239  0.0118218101  4200          0.3062431264 
0.8206223307  0.8557457213  0.9989339019  0.9551282051  0.9985029940  0.9670658683  0.9971374046  0.9490445860  107.78443113  0.0083560429  4500          0.3066036995 
0.8517388652  0.8557457213  0.9978678038  0.9508547009  1.0000000000  0.9730538922  0.9980916031  0.9554140127  114.97005988  0.0091357930  4800          0.3063246751 
0.8090298963  0.8361858191  0.9989339019  0.9679487179  0.9977544910  0.9610778443  0.9968193384  0.9566878981  119.76047904  0.0102657288  5000          0.3059591365 
